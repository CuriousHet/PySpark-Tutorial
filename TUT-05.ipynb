{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bf1dd7-cfe1-42b4-847a-01faa4c44a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c6de5ae2f65e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Joins and Data Partitions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc266d6c970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Joins and Data Partitions\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddd7bdd-9fcf-4725-bc89-c13cb1ecb2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+---+------+------+----------+\n",
      "|employee_id|department_id|name|age|gender|salary|      date|\n",
      "+-----------+-------------+----+---+------+------+----------+\n",
      "|        E01|          D01|John| 30|     M|  3000|2022-01-01|\n",
      "|        E02|          D02|Jane| 25|     F|  4000|2022-02-15|\n",
      "|        E03|          D01|Jake| 28|     M|  3500|2022-03-10|\n",
      "|        E04|          D03|Jill| 35|     F|  5000|2022-04-05|\n",
      "|        E05|          D02|Bill| 29|     M|  3800|2022-05-20|\n",
      "+-----------+-------------+----+---+------+------+----------+\n",
      "\n",
      "+-------------+---------------+-------------+-------+------+\n",
      "|department_id|department_name|         city|country|budget|\n",
      "+-------------+---------------+-------------+-------+------+\n",
      "|          D01|             HR|     New York|    USA|100000|\n",
      "|          D02|    Engineering|San Francisco|    USA|200000|\n",
      "|          D03|          Sales|       London|     UK|150000|\n",
      "+-------------+---------------+-------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, date string\"\n",
    "\n",
    "emp_data = [\n",
    "    (\"E01\", \"D01\", \"John\", \"30\", \"M\", \"3000\", \"2022-01-01\"),\n",
    "    (\"E02\", \"D02\", \"Jane\", \"25\", \"F\", \"4000\", \"2022-02-15\"),\n",
    "    (\"E03\", \"D01\", \"Jake\", \"28\", \"M\", \"3500\", \"2022-03-10\"),\n",
    "    (\"E04\", \"D03\", \"Jill\", \"35\", \"F\", \"5000\", \"2022-04-05\"),\n",
    "    (\"E05\", \"D02\", \"Bill\", \"29\", \"M\", \"3800\", \"2022-05-20\")\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data, schema=emp_schema)\n",
    "\n",
    "emp_df.show()\n",
    "\n",
    "dept_schema = \"department_id string, department_name string, city string, country string, budget string\"\n",
    "\n",
    "dept_data = [\n",
    "    (\"D01\", \"HR\", \"New York\", \"USA\", \"100000\"),\n",
    "    (\"D02\", \"Engineering\", \"San Francisco\", \"USA\", \"200000\"),\n",
    "    (\"D03\", \"Sales\", \"London\", \"UK\", \"150000\")\n",
    "]\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265983a8-8cb1-4927-91f2-a48f96d7e6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882c1ed1-f1b0-4e90-8533-b2a62999ef9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269d345f-d251-400c-a88d-a0dd8e441e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition -> + - partitions\n",
    "# it includes data shuffling \n",
    "\n",
    "emp_par = emp_df.repartition(40)\n",
    "emp_par.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60952a8-8cea-4a9f-80b8-3409071f49aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce -> - partions\n",
    "# it doesnt include data shuffling \n",
    "\n",
    "emp_par = emp_df.repartition(20)\n",
    "emp_par.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52cfc0f6-a16c-4bcb-a867-6f42ad623e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_part = emp_df.repartition(4,\"department_id\")\n",
    "emp_part.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac2f1c2-2005-4170-bfe1-c188795943ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "|employee_id|department_id|name|age|gender|salary|      date|partition_num|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "|        E01|          D01|John| 30|     M|  3000|2022-01-01|            3|\n",
      "|        E02|          D02|Jane| 25|     F|  4000|2022-02-15|            7|\n",
      "|        E03|          D01|Jake| 28|     M|  3500|2022-03-10|           11|\n",
      "|        E04|          D03|Jill| 35|     F|  5000|2022-04-05|           15|\n",
      "|        E05|          D02|Bill| 29|     M|  3800|2022-05-20|           19|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# partition info\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "emp1 = emp_df.withColumn(\"partition_num\",spark_partition_id())\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b6caa4-4255-4d70-b1c7-77e45d0b87cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "|employee_id|department_id|name|age|gender|salary|      date|partition_num|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "|        E01|          D01|John| 30|     M|  3000|2022-01-01|            0|\n",
      "|        E03|          D01|Jake| 28|     M|  3500|2022-03-10|            0|\n",
      "|        E02|          D02|Jane| 25|     F|  4000|2022-02-15|            1|\n",
      "|        E04|          D03|Jill| 35|     F|  5000|2022-04-05|            1|\n",
      "|        E05|          D02|Bill| 29|     M|  3800|2022-05-20|            1|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp_df.repartition(4,\"department_id\").withColumn(\"partition_num\",spark_partition_id())\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dde5cc9-6344-481b-b32f-972c8f8dda61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "|employee_id|department_id|name|age|gender|salary|      date|department_id|department_name|         city|country|budget|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "|        E01|          D01|John| 30|     M|  3000|2022-01-01|          D01|             HR|     New York|    USA|100000|\n",
      "|        E03|          D01|Jake| 28|     M|  3500|2022-03-10|          D01|             HR|     New York|    USA|100000|\n",
      "|        E02|          D02|Jane| 25|     F|  4000|2022-02-15|          D02|    Engineering|San Francisco|    USA|200000|\n",
      "|        E05|          D02|Bill| 29|     M|  3800|2022-05-20|          D02|    Engineering|San Francisco|    USA|200000|\n",
      "|        E04|          D03|Jill| 35|     F|  5000|2022-04-05|          D03|          Sales|       London|     UK|150000|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join datasets\n",
    "# select e.emp_name, d.dept_name, d.department_id, e.salary\n",
    "# from emp e inner join dept d on emp.department_id = dept.department_id\n",
    "\n",
    "df_joined = emp_df.alias(\"e\").join(dept_df.alias(\"d\" ), how=\"inner\", on=emp_df.department_id==dept_df.department_id)\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd23fbcd-cd08-4be0-b4ae-01b939ec4c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------+------+\n",
      "|name|department_id|department_name|salary|\n",
      "+----+-------------+---------------+------+\n",
      "|John|          D01|             HR|  3000|\n",
      "|Jake|          D01|             HR|  3500|\n",
      "|Jane|          D02|    Engineering|  4000|\n",
      "|Bill|          D02|    Engineering|  3800|\n",
      "|Jill|          D03|          Sales|  5000|\n",
      "+----+-------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.select(\"e.name\", \"d.department_id\", \"d.department_name\", \"e.salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f21af1-e3fa-4b0e-a640-0fdceec93034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "|employee_id|department_id|name|age|gender|salary|      date|department_id|department_name|         city|country|budget|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "|        E01|          D01|John| 30|     M|  3000|2022-01-01|          D01|             HR|     New York|    USA|100000|\n",
      "|        E02|          D02|Jane| 25|     F|  4000|2022-02-15|          D02|    Engineering|San Francisco|    USA|200000|\n",
      "|        E03|          D01|Jake| 28|     M|  3500|2022-03-10|          D01|             HR|     New York|    USA|100000|\n",
      "|        E04|          D03|Jill| 35|     F|  5000|2022-04-05|          D03|          Sales|       London|     UK|150000|\n",
      "|        E05|          D02|Bill| 29|     M|  3800|2022-05-20|          D02|    Engineering|San Francisco|    USA|200000|\n",
      "+-----------+-------------+----+---+------+------+----------+-------------+---------------+-------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left outer join datasets\n",
    "# select e.emp_name, d.department_id, e.salary\n",
    "\n",
    "df_joined = emp_df.alias(\"e\").join(dept_df.alias(\"d\"), how=\"left_outer\", on=emp_df.department_id==dept_df.department_id)\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766869c-c5ea-4a23-b4fb-7fc46848c8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
